<h1 align="center">Предсказание продолжительности поездки такси в Нью-Йорке</h1>
<p align="center">
  <strong>ML практический практикум</strong>
</p>

<h2>1. Постановка задачи</h2>

<p>В данном проекте мы решаем задачу машинного обучения, связанную с прогнозированием общей продолжительности поездки такси в Нью-Йорке. Наша модель будет использовать различные характеристики поездки, чтобы предсказать ее продолжительность. Это поможет автоматизировать бизнес-процессы и оценивать стоимость поездки.</p>
<p>Знакомство с данными, базовый анализ и расширение данных:
В этой части проекта мы начинаем с загрузки и базового анализа предоставленных данных. Мы импортируем необходимые модули и считываем файл с данными, содержащим около 1.5 миллиона поездок и 11 характеристик для каждой поездки.</p>
<p>Далее мы рассматриваем различные группы признаков, такие как данные о клиенте и таксопарке, временные характеристики и географическая информация, а также другие признаки, включая целевой признак - продолжительность поездки.</p>
<p>Мы проводим базовый анализ данных, чтобы оценить их готовность для дальнейшей обработки и анализа. Мы преобразуем столбец pickup_datetime в формат даты и времени и определяем временные рамки представленных данных. Затем мы проверяем наличие пропущенных значений во всех столбцах таблицы и вычисляем статистические характеристики некоторых признаков. </p>

<p>Далее мы расширяем исходный набор данных, добавляя новые признаки. Мы реализуем функцию add_datetime_features(), которая добавляет столбцы с датой, часом и днем недели начала поездки. Мы также реализуем функцию add_holiday_features(), которая добавляет столбец с бинарным признаком, указывающим, была ли поездка начата в праздничный день. </p>

<p>Задание 2.6:</p>
<p>В данном задании была реализована функция add_osrm_features(), которая объединяет таблицу с данными о поездках и таблицу с данными из OSRM (Open Source Routing Machine). Функция добавляет в таблицу с данными о поездках три новых столбца: total_distance (общее расстояние поездки), total_travel_time (общее время поездки) и number_of_steps (количество шагов в маршруте). Для объединения таблиц используется операция слияния (merge). </p>

<p>а) Разница (в секундах) между медианной длительностью поездки в данных и медианной длительностью поездки, полученной из OSRM, вычисляется как: taxi_data_extended3.trip_duration.median() - taxi_data_extended3.total_travel_time.median().</p> Результат показывает, насколько различаются медианные значения длительности поездки в данных и в OSRM.</p>

<p>б) Количество пропусков в столбцах с информацией из OSRM API после объединения таблиц можно вычислить с помощью функции isnull().sum() для соответствующих столбцов в taxi_data_extended3.</p>

<p>Задание 2.7:</p>
<p>В задании 2.7 была реализована функция add_geographical_features(), которая добавляет в таблицу с данными о поездках два новых столбца: haversine_distance (расстояние Хаверсина между точками начала и конца поездки) и direction (направление движения из точки начала в точку конца поездки). Для вычисления расстояния и направления используются функции get_haversine_distance() и get_angle_direction() соответственно.</p>

<p>Медианное расстояние Хаверсина поездок вычисляется как: round(taxi_data_extended4.haversine_distance.median(), 2).</p>

<p>Задание 2.9:</p>
<p>Функция add_weather_features() принимает на вход таблицу с данными о поездках и таблицу с данными о погодных условиях на каждый час и добавляет в таблицу с поездками пять новых столбцов с информацией о погоде: temperature (температура), visibility (видимость), wind speed (средняя скорость ветра), precip (количество осадков) и events (погодные явления).</p>

<p>Задание 3.1:</p>
В задании 3.1 была проведена визуализация распределения длительности поездок в логарифмическом масштабе (trip_duration_log) с помощью гистограммы и коробчатой диаграммы (ящика с усами).</p>

<p>а) Для проверки гипотезы о нормальности распределения был использован тест Д’Агостино при уровне значимости α=0.05. Результаты теста, включая p-value, были вычислены с помощью функции normaltest() из модуля scipy.stats. Значение p-value было округлено до сотых.</p>

<p>б) Для определения является ли распределение длительности поездок в логарифмическом масштабе нормальным, было проанализировано значение p-value. Если p-value меньше выбранного уровня значимости α=0.05, то гипотеза о нормальности распределения отклоняется.</p>

<p>Задание 3.2:</p>
<p>В задании 3.2 была построена визуализация, позволяющая сравнить распределение длительности поездок в логарифмическом масштабе (trip_duration_log) в зависимости от таксопарка (vendor_id). Были построены две гистограммы, отображающие распределение для каждого таксопарка. Распределения были сравнены между собой.</p>

<p>Задание 3.3:</p>
<p>В задании 3.3 была построена визуализация, позволяющая сравнить распределение длительности поездок в логарифмическом масштабе (trip_duration_log) в зависимости от признака отправки сообщения поставщику (store_and_fwd_flag). Были построены две гистограммы, отображающие распределение для каждого значения признака. Распределения были сравнены между собой.</p>

<p>Визуализация данных позволяет сделать предположения и получить первичные выводы о распределении длительности поездок в логарифмическом масштабе и его зависимости от других переменных, таких как таксопарк или признак отправки сообщения поставщику.</p>

<p>Задание 3.4:</p>
<p>В задании 3.4 были построены две визуализации, которые помогли изучить зависимость количества поездок и медианной длительности поездок от часа дня.</p>

<p>а) На первом графике было построено распределение количества поездок в зависимости от часа дня. Из графика видно, что наибольшее количество поездок приходится на определенные промежутки времени, а наименьшее количество поездок — на другие промежутки. Ответом на вопрос о времени суток, когда такси заказывают реже всего, является временной интервал с наименьшим количеством поездок.</p>

<p>б) На втором графике была показана зависимость медианной длительности поездок от часа дня. Из графика можно сделать вывод о времени суток, когда наблюдается пик медианной длительности поездок — это часы с наибольшим значением медианной длительности.</p>

<p>Задание 3.5:</p>
<p>В задании 3.5 были построены две визуализации, которые позволили изучить распределение количества поездок и зависимость медианной длительности поездок от дня недели.</p>

<p>а) На первом графике было построено распределение количества поездок в зависимости от дня недели. Из графика можно определить день недели, когда совершается наибольшее количество поездок — это день с наибольшим значением количества поездок.</p>

<p>б) На втором графике была показана зависимость медианной длительности поездок от дня недели. Из графика можно сделать вывод о дне недели, когда наблюдается наименьшая медианная длительность поездок.</p>

<p>Задание 3.6:</p>
<p>В задании 3.6 была построена сводная таблица, в которой по строкам отложены часы дня (pickup_hour), по столбцам - дни недели (pickup_day_of_week), а в ячейках - медианная длительность поездки (trip_duration). Для визуализации таблицы была использована тепловая карта с палитрой coolwarm.</p>

<p>Сводная таблица и тепловая карта позволяют наглядно оценить взаимосвязь между часом дня, днем недели и медианной длительностью поездок. Цветовая шкала на тепловой карте отображает значения медианной длительности, где более тёплые цвета соответствуют более высоким значениям, а более холодные цвета — более низким значениям.</p>

<p>С помощью тепловой карты можно увидеть, как длительность поездок меняется в зависимости от часа дня и дня недели. Например, можно определить пики и провалы в длительности поездок в различные дни и часы, что может быть полезно для анализа спроса и планирования ресурсов.</p>

<p>Общая информация о данных и визуализациях, представленных в заданиях 3.1-3.6, помогают лучше понять характеристики и зависимости переменных в исследуемом наборе данных о поездках такси.</p>


<p>Задание 3.7 требовало построить две диаграммы рассеяния, иллюстрирующие географическое расположение точек начала и завершения поездок в Нью-Йорке. Для этого были использованы координаты широты (latitude) и долготы (longitude). Ограничения на границы осей абсцисс и ординат были заданы следующим образом:</p>

<p>city_long_border = (-74.03, -73.75)</p>
<p>city_lat_border = (40.63, 40.85)</p>

<p>На первой диаграмме было показано географическое расположение точек начала поездок (pickup_longitude, pickup_latitude), а на второй - точек завершения поездок (dropoff_longitude, dropoff_latitude). Точки, которые находятся в пределах Нью-Йорка, были выделены на графиках.</p>

<p>Для обеспечения наглядности, на диаграммах использовалась расцветка по десяти географическим кластерам (geo_cluster), которые были сгенерированы ранее. Для уменьшения размера точек на диаграмме был использован параметр "s=8".</p>

<p>В результате полученные диаграммы рассеяния позволяют визуализировать географическое распределение начальных и конечных точек поездок в Нью-Йорке с учетом географических кластеров. Это помогает лучше понять паттерны и характеристики перемещений в различных областях города.</p>
<p>Задание 4.1:</p>
<p>а) Вопрос гласит, какой из признаков является уникальным для каждой поездки и не несет полезной информации в определении ее продолжительности. Ответ: признак "id".</p>
<p>б) Утечка данных (data leak) - это ситуация, когда информация о целевой переменной (trip_duration) содержится в признаках, доступных на момент обучения модели, что может привести к завышенной точности предсказания.</p>

<p>г) Признак "dropoff_datetime" создает утечку данных, так как он содержит информацию о времени окончания поездки, которая неизвестна на момент предсказания продолжительности.</p>

<p>Затем выбранные признаки были исключены из исходной таблицы с данными. В результате осталось определенное количество столбцов.</p>

<p>Задание 4.2:</p>
<p>а) Признак "vendor_id" был закодирован таким образом, чтобы он был равен 1, если идентификатор таксопарка равен 0, и 1 в противном случае. Среднее значение закодированного столбца "vendor_id" составляет 0.53 с точностью до сотых.</p>
<p>б) Признак "store_and_fwd_flag" был закодирован таким образом, чтобы он был равен 0, если флаг выставлен в значение "N", и 0 в противном случае. Среднее значение закодированного столбца "store_and_fwd_flag" составляет 0.006 с точностью до тысячных.</p>

<p>Задание 4.3:</p>
<p>Для кодирования признаков "pickup_day_of_week", "geo_cluster" и "events" был использован класс OneHotEncoder из библиотеки sklearn. Параметр "drop" был установлен в значение "first", чтобы удалить первый бинарный столбец и избежать создания излишних признаков. Полученный numpy-массив был преобразован обратно в DataFrame под названием "data_onehot" с использованием закодированных имен столбцов.</p>

<p>Количество бинарных столбцов, сгенерированных с помощью однократного кодирования, зависит от количества уникальных категорий в каждом из признаков. Данная информация не предоставлена в тексте задания, поэтому точное количество сгенерированных бинарных столбцов неизвестно.</p>

<p>В результате выполнения задания был сформирован DataFrame "train_data", который содержит кодированные категориальные признаки. Форма "train_data" указывает количество столбцов после объединения кодированных признаков и удаления некоторых исходных признаков.</p>




<p>Задание 4.4:</p>
<p>Для отбора наиболее подходящих признаков для предсказания целевой переменной в логарифмическом масштабе была использована методика SelectKBest из модуля feature_selection библиотеки sklearn. Было выбрано 25 лучших признаков с помощью параметра k = 25 и функции оценки score_func = f_regression.</p>

<p>После обучения SelectKBest на обучающей выборке с помощью метода fit, были получены отобранные признаки с помощью атрибута feature_names_in_. Полученный список включает следующие признаки:</p>
<p> ['vendor_id', 'passenger_count', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'store_and_fwd_flag', 'pickup_hour', 'pickup_holiday', 'total_distance', 'total_travel_time', 'number_of_steps', 'haversine_distance', 'direction', 'temperature', 'visibility', 'wind speed', 'precip', 'pickup_day_of_week_0', 'pickup_day_of_week_1', 'pickup_day_of_week_2', 'pickup_day_of_week_3', 'pickup_day_of_week_4', 'pickup_day_of_week_5', 'pickup_day_of_week_6', 'geo_cluster_0', 'geo_cluster_1', 'geo_cluster_2', 'geo_cluster_3', 'geo_cluster_4', 'geo_cluster_5', 'geo_cluster_6', 'geo_cluster_7', 'geo_cluster_8', 'geo_cluster_9', 'events_Fog', 'events_None', 'events_Rain', 'events_Snow', 'events_nan']</p>

<p>Задание 4.5:</p>
<p>Для подготовки данных перед обучением моделей было выполнено нормализация предикторов в обучающей и валидационной выборках с помощью MinMaxScaler из библиотеки sklearn. При этом, важно отметить, что обучение нормализатора производится только на обучающей выборке, а затем применяется трансформация как на обучающей, так и на валидационной выборке.</p>

<p>С использованием метода fit_transform нормализатора scaler, были преобразованы обучающая и валидационная выборки в scaled_train и scaled_valid соответственно.</p>

<p>Далее, было рассчитано среднее арифметическое для первого предиктора (первого столбца матрицы) из валидационной выборки с помощью выражения scaled_valid[:,0].mean(). Полученный результат округлен до сотых и равен 0.54.</p>

<p>Дополнительно, было представлено распределение значений признака "vendor_id" из обучающей выборки, которое показывает количество записей со значением 1 (522458) и 0 (454558).</p>

<p>Рекоментдации к выполнению проекта: Выполняйте ячейки блоками, все сразу он захлебывается делать
Блок 3 с EDA можно пропустить при обучении моделей, он поджирает оперативку, а толку от него для моделек нет

Кроме того, вы сэкономите кучу оперативы, если сохраните уже обработанный набор тренировочных и валидационных данных на диск. Так вы сможете не выполнять бОльшую часть блока 2 с загрузкой исходного датасета на 1.5 ляма строк и сможете пропустить весь 4 блок.

После получения готовых датасетов для моделек в конце блока 4 выполните примерно следующий код, который сохраняет данные на диск:

X_train.to_csv(f"{PROCESSED_DATA_PATH}/X_train.csv", index=False)
X_valid.to_csv(f"{PROCESSED_DATA_PATH}/X_valid.csv", index=False)
np.save(f"{PROCESSED_DATA_PATH}/y_train_log", y_train_log)
np.save(f"{PROCESSED_DATA_PATH}/y_valid_log", y_valid_log)

Потом перезагрузите среду в колабе, выполните импорты и подключение гуглдрайва из блока 2, в начале блока 5 загрузите датасеты с помощью примерно вот такого кода:

X_train = pd.read_csv(f"{PROCESSED_DATA_PATH}/X_train.csv")
X_valid = pd.read_csv(f"{PROCESSED_DATA_PATH}/X_valid.csv")
y_train_log = np.load(f"{PROCESSED_DATA_PATH}/y_train_log.npy")
y_valid_log = np.load(f"{PROCESSED_DATA_PATH}/y_valid_log.npy")

И тогда вы сможете выполнить все задания по моделькам в гугл колабе без утечек памяти (даже 5.1 и 5.2)

Тот же прием можете выполнить и для обработки тестовых данных :)</p>